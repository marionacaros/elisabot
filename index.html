
<!DOCTYPE HTML>
<html>
	<head>
		<title>Automatic Reminiscence Therapy for Dementia</title>
		<meta http-equiv="content-type" content="text/html; charset=utf-8" />
		<meta name="description" content="" />
		<meta name="keywords" content="" />
		<!--[if lte IE 8]><script src="js/html5shiv.js"></script><![endif]-->
		<script src="js/jquery.min.js"></script>
		<script src="js/skel.min.js"></script>
		<script src="js/skel-layers.min.js"></script>
		<script src="js/init.js"></script>
		<noscript>
			<link rel="stylesheet" href="css/skel.css" />
			<link rel="stylesheet" href="css/style.css" />
			<link rel="stylesheet" href="css/style-xlarge.css" />
		</noscript>
	</head>
	<body id="top">

		<!-- Header -->
		<header id="header" class="skel-layers-fixed">
			<h1><a href="#"></a></h1>
			<nav id="nav">
				<ul>
					<li><a href="#video">Demo</a></li>
					<li><a href="#architecture">Architecture</a></li>
					<li><a href="#author">Authors</a></li>
				</ul>
			</nav>
		</header>

		<!-- Banner -->
		<section id="banner">
			<div class="inner">
				<h2></h2>
			</div>
		</section>

		<!-- One -->
		<section id="video" class="wrapper style1">
			<header class="major">
			</header>
			<div class="container">
				<div class="row" >
						<div class="6u">
							<h2>DEMO</h2>
							<section>
							<video poster="screen-elisabot.png" width="400" height="800" controls preload>
							    <source src="demo-Elisabot.mp4" media="only screen and (min-device-width: 568px)"></source> 
							    <source src="demo-Elisabot.iphone.mp4" media="only screen and (max-device-width: 568px)"></source> 
							</video>
							</section>
						</div>
						<div class="6u">
							<h2>Methodology</h2>
							<section>
								<p> Elisabot is a conversational agent that simulates a reminiscence therapist by asking questions 
								about the patient's experiences. Questions are generated from pictures provided by the patient, 
								which contain significant moments or important people in user's life. The proposed methodology is 
								specific for dementia therapy, compared to a general Image-based Question and Answering (Q&A) 
								system, because the generated questions cannot be answered by only looking at the picture as 
								common Q&A systems do, the user needs to know the place, the time, the people or animals appearing 
								in the picture to be able to answer the questions. The activity pretends to be challenging 
								for the patient, as the questions may require the user to exercise the memory, but amusing at the 
								same time. 
								
								</p>
								<a class="image fit"><img src="images/diagrama.png" alt="" /></a>
								<p>
								 Before starting the conversation, the user must introduce photos containing 
							significant moments for him/her. The system randomly chooses one of these pictures and analyses 
							the content. Then, Elisabot shows the selected picture and starts the conversation by asking a 
							question about the picture. The user should give an answer, even though he does not know it, 
							and Elisabot makes a relevant comment on it. The cycle starts again by asking another relevant 
							question about the image and the flow is repeated for 4 to 6 times until the picture is changed. 

<!-- 								Our contributions include:

								- Automation of the Reminiscence therapy by using a multi-modal approach that generates questions 
								from pictures, without using a reminiscence therapy dataset.
    
								- An end-to-end deep learning approach which do not require hand-crafted rules and it is ready to 
								be used by mild cognitive impairment patients. The system is designed to be intuitive and easy to 
								use for the users and could be reached by any smartphone with internet connection.
								 -->
								</p>
							</section>
						</div>
				

					</section>
				</div>
			</div>
		</section>

		<!-- Two -->
<!-- 		<section id="methodology" class="wrapper style1">
			<header class="major">
				<h2>Methodology</h2>
			</header>
			<div class="container">
				<div class="row">
					<div  align="center">
						<section>
							<h2>Interaction with Elisabot</h2>
							<p></p>
						</section>
					</div>
				</div>
			</div>
		</section>	 -->
		
		<!-- Three -->
		<section id="architecture" class="wrapper style1">
			<header class="major">
				<h2>Architecture</h2>
			</header>
			<div class="container">
				<p> Elisabot is composed of two models: 
					the model in charge of asking questions about the image which we will refer to it as Visual Question Generator (VQG),
					and the Chatbot model which tries to make the dialogue more engaging by giving feedback to the user's answers.
				</p>
				<div class="row">
					<div class="6u">
						<section>
							<h2>VQG model</h2>
							<a class="image fit"><img src="images/VQG_figure.png" alt="" /></a>
							<p> 
								The algorithm behind VQG consists in an Encoder-Decoder architecture with attention.
								The model is trained to maximize the likelihood 
								of producing a target sequence of words optimizing the cross-entropy loss. 
								The Encoder takes as input one of the given photos from the user and learns its information using a 
								Convolutional Neural Network (CNN). The CNN provides the image's learned features to the Decoder which
								generates the question word by word by using an attention mechanism with a Long Short-Term Memory (LSTM).
								Since there are already CNNs trained on large datasets with an outstanding performance, 
								we integrate a ResNet-101 trained on ImageNet. 

							</p>
						</section>
					</div>
					<div class="6u">
						<section>
							<h2>Chatbot model</h2>
							<a class="image fit"><img src="images/seq-to-seq.png" alt="" /></a>
							<p> The core of our chatbot model is a sequence-to-sequence. The encoder iterates through the input sentence
								one word at each time step producing an output vector and a hidden state vector. The hidden state 
								vector is passed to the next time step, while the output vector is stored. We use a bidirectional 
								Gated Recurrent Unit (GRU), one GRU fed in sequential order and another one fed in reverse order. 
								The outputs of both networks are summed at each time step, so we encode past and future context. 

								By using an attention mechanism, the decoder uses the encoder’s context vectors, and internal hidden
								states to generate the next word in the sequence. 
								It continues generating words until it outputs an token. We use an attention layer to multiply 
								attention weights to encoder's outputs to focus on the relevant information when decoding the sequence.
								This approach have shown better performance on sequence-to-sequence models
							</p>
						</section>
					</div>
					
				</div>

			</div>
		</section>
		<!-- Four -->
		<section id="author" class="wrapper style1">
			<header class="major">
				<h3>main author</h3>
			</header>
			<div align="center">
				Mariona Carós
			</div>
			<div class="row">
			</div>

			<header class="major">
				<h4>Contributors</h4>
			</header>
				<div align="center">
					Xavier Giró-i-Nieto, Petia Radeva, Maite Garolera
				</div>
			
		</section>	
		
	</body>
</html>
