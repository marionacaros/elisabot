
<!DOCTYPE HTML>
<html>
	<head>
		<title>Automatic Reminiscence Therapy for Dementia</title>
		<meta http-equiv="content-type" content="text/html; charset=utf-8" />
		<meta name="description" content="" />
		<meta name="keywords" content="" />
		<!--[if lte IE 8]><script src="js/html5shiv.js"></script><![endif]-->
		<script src="js/jquery.min.js"></script>
		<script src="js/skel.min.js"></script>
		<script src="js/skel-layers.min.js"></script>
		<script src="js/init.js"></script>
		<noscript>
			<link rel="stylesheet" href="css/skel.css" />
			<link rel="stylesheet" href="css/style.css" />
			<link rel="stylesheet" href="css/style-xlarge.css" />
		</noscript>
	</head>
	<body id="top">

		<!-- Header -->
		<header id="header" class="skel-layers-fixed">
			<h1><a href="#"></a></h1>
			<nav id="nav">
				<ul>
					<li><a href="#video">Video</a></li>
					<li><a href="#methodology">Methodology</a></li>
 					<li><a href="#dataset">Dataset</a></li>
					<li><a href="#architecture">Architecture</a></li>
				</ul>
			</nav>
		</header>

		<!-- Banner -->
		<section id="banner">
			<div class="inner">
				<h2></h2>
			</div>
		</section>

		<!-- One -->
		<section id="video" class="wrapper style1">
			<header class="major">
				<h2>Video</h2>
			</header>
			<div class="container">
				<div class="row">
					<div class="12u">
						<section class="special box">
							<video src="demo-Elisabot.mp4" width="640" height="400" controls preload></video>

						</section>
					</div>
				</div>
			</div>
		</section>

		<!-- Three -->
		<section id="methodology" class="wrapper style1">
			<header class="major">
				<h2>Methodology</h2>
			</header>
			<div class="container">
				<div class="row">
					<div class="6u">
						<section>
							<h2>Interaction with Elisabot</h2>
							<p> We named the model Elisabot and its goal is to maintain a dialog with the patient about userâ€™s 
							life experiences. Before starting the conversation, the user must introduce photos containing 
							significant moments for him/her. The system randomly chooses one of these pictures and analyses 
							the content. Then, Elisabot shows the selected picture and starts the conversation by asking a 
							question about the picture. The user should give an answer, even though he does not know it, 
							and Elisabot makes a relevant comment on it. The cycle starts again by asking another relevant 
							question about the image and the flow is repeated for 4 to 6 times until the picture is changed. 
							</p>
						</section>
					</div>
					<div class="6u">	
						<section>
							<a class="image fit"><img src="images/diagrama.png" alt="" /></a>
							<p></p>
						</section>
					</div>
				</div>
			</div>
		</section>			

		<section id="architecture" class="wrapper style1">
			<header class="major">
				<h2>Architecture</h2>
			</header>
			<div class="container">
				<p> Elisabot is composed of two models: 
					the model in charge of asking questions about the image which we will refer to it as Visual Question Generator (VQG),
					and the Chatbot model which tries to make the dialogue more engaging by giving feedback to the user's answers.
				</p>
				<div class="row">
					<div class="6u">
						<section>
							<h2>VQG model</h2>
							<a class="image fit"><img src="images/VQG_figure (1).png" alt="" /></a>
							<p> 
								The algorithm behind VQG consists in an Encoder-Decoder architecture with attention, 
								as shown in Figure \ref{fig:vqg_model}. The model is trained to maximize the likelihood 
								of producing a target sequence of words optimizing the cross-entropy loss \cite{bishop2006pattern}. 
								The Encoder takes as input one of the given photos from the user and learns its information using a 
								Convolutional Neural Network (CNN). The CNN provides the image's learned features to the Decoder which
								generates the question word by word by using an attention mechanism with a Long Short-Term Memory (LSTM).
								Since there are already CNNs trained on large datasets with an outstanding performance, 
								we integrate a \textit{ResNet-101} \cite{he2016deep} trained on ImageNet. 

 Regarding hyperparameters, the VQG encoder is composed of 2048 neuron cells, while the VQG decoder has an attention layer of 512 followed by an embedding layer of 512 and a LSTM with the same size. We set the batch size to 32. We use a dropout of 50\% and a beam search of 7 for decoding, which let as obtain up to 5 output questions. The vocabulary we use consists of all words seen 3 or more times in the training set, which amounts to 11.214 unique tokens. Unknown words are mapped to an $<$\texttt{unk}$>$ token during training, but we do not allow the decoder to produce this token at test time. We also set a maximum sequence length of 6 words as we want simple questions easy to understand and easy to learn by the model.
							</p>
						</section>
					</div>
					<div class="6u">
						<section>
							<h2>Chatbot model</h2>
							<a class="image fit"><img src="images/seq-to-seq.png" alt="" /></a>
							<p> 
							</p>
						</section>
					</div>
					
				</div>

			</div>
		</section>

		
	</body>
</html>
